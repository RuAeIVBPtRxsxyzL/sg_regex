{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from itertools import product\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "from lark import Lark, Transformer, v_args\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from stellargraph.data import UniformRandomWalk, BiasedRandomWalk, UniformRandomMetaPathWalk\n",
    "from stellargraph import StellarGraph, StellarDiGraph, datasets\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expression grammar meant for creating succinct patterns through a graph. The grammar below must contain a NODE Identifier, which should correspond to the node label in the Neo4J database. \n",
    "\n",
    "Examples of valid grammar statements:\n",
    "\n",
    "<i>\"chemical_substance treats> disease\"</i> -> this would search for any node with the label <b>chemical_substance</b> connected to nodes of type <b>disease</b> with an edge of type <i>treats</i>.\n",
    "\n",
    "<i>\"chemical_substance ? disease\"</i> -> this would search for any node with the label <b>chemical_substance</b> connected to nodes of type <b>disease</b> with any edge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_grammar = \"\"\"\n",
    "    start: node \n",
    "         | node (edge node)+\n",
    "         | node \"(\" path \")\"\n",
    "\n",
    "       \n",
    "    ?path: edge_node\n",
    "         | path \"|\" edge_node   -> path_or \n",
    "    \n",
    "    ?edge_node: edge node\n",
    "         | \"(\" edge_node+ \")\" \n",
    "         \n",
    "\n",
    "         \n",
    "    ?edge: attm\n",
    "         | edge \"|\" attm        -> edge_or\n",
    "         \n",
    "    ?attm: EDGE_LABEL           -> edge\n",
    "         | attm \">\"             -> edge_right\n",
    "         | attm \"<\"             -> edge_left\n",
    "         | NULL                 -> edge_no_label\n",
    "         | \"(\" edge \")\"     \n",
    "    \n",
    "    ?node: atom\n",
    "        | node \"|\" atom         -> node_or\n",
    "\n",
    "    ?atom: NODE_LABEL           -> node\n",
    "         | NODE_LABEL \"*\"       -> rep_from_0\n",
    "         | NODE_LABEL \"+\"       -> rep_from_1\n",
    "         | NULL                 -> node_no_label\n",
    "         | \"(\" node \")\"\n",
    "\n",
    "    EDGE_LABEL: LABEL_STRING\n",
    "    NODE_LABEL: LABEL_STRING\n",
    "    NULL: \"?\"\n",
    "    LCASE_LETTER: \"a\"..\"z\"\n",
    "    UCASE_LETTER: \"A\"..\"Z\"\n",
    "    DIGIT: \"0\"..\"9\"\n",
    "\n",
    "    LETTER: UCASE_LETTER | LCASE_LETTER | DIGIT | \"_\" | \"-\"\n",
    "    LABEL_STRING: LETTER+ | \"_\" \n",
    "\n",
    "    %import common.CNAME -> NAME\n",
    "    %import common.WS_INLINE\n",
    "    %ignore WS_INLINE\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lark parser. Converts a subgraph regular expression into a CYPHER query. These functions act on different triggers provided by the grammar above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@v_args(inline=True)    # Affects the signatures of the methods\n",
    "class CalculateTree(Transformer):\n",
    "    node_idx = 0\n",
    "    edge_idx = 0\n",
    "        \n",
    "    def node(self, name):\n",
    "        self.node_idx += 1\n",
    "        return \"(n{}:\".format(self.node_idx) + str(name) +\")\"\n",
    "    \n",
    "    def edge(self, name):\n",
    "        self.edge_idx += 1\n",
    "        return \"-[r{}:\".format(self.edge_idx) + str(name) +\"]-\"\n",
    "    \n",
    "    def node_no_label(self, name):\n",
    "        self.node_idx += 1\n",
    "        return \"(n{}\".format(self.node_idx) +\")\"\n",
    "    \n",
    "    def edge_no_label(self, name):\n",
    "        self.edge_idx += 1\n",
    "        return \"-[r{}\".format(self.edge_idx) +\"]-\"\n",
    "    \n",
    "    def edge_node(self, name1, name2):\n",
    "        path = name1 + name2\n",
    "        return path\n",
    "\n",
    "    def edge_right(self, name):\n",
    "        path = ''\n",
    "        path = name \n",
    "        return path + \">\"\n",
    "\n",
    "    def edge_left(self, name):\n",
    "        path = ''\n",
    "        path = name \n",
    "        return \"<\" + path\n",
    "\n",
    "    def rep_from_0(self, name):\n",
    "        self.node_idx += 1\n",
    "        path1 = \"(n{}:\".format(self.node_idx) + str(name) +\")\"\n",
    "        path2 = \"(n{}:\".format(self.node_idx) + str(name) +\")\" + \"--\" + \"(n{}:\".format(self.node_idx+1) + str(name) +\")\"\n",
    "        self.node_idx += 1\n",
    "        return ['', path1, path2]\n",
    "\n",
    "\n",
    "    def rep_from_1(self, name):\n",
    "        self.node_idx += 1\n",
    "        path1 = \"(n{}:\".format(self.node_idx) + str(name) +\")\"\n",
    "        path2 = \"(n{}:\".format(self.node_idx) + str(name) +\")\" + \"--\" + \"(n{}:\".format(self.node_idx+1) + str(name) +\")\"\n",
    "        self.node_idx += 1\n",
    "        return [path1, path2]\n",
    "\n",
    "    def node_or(self, name1, name2):\n",
    "        return [name1, name2]\n",
    "    \n",
    "    def edge_or(self, name1, name2):\n",
    "        return [name1, name2]\n",
    "    \n",
    "    def path_or(self, name1, name2):\n",
    "        return [name1, name2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permutes all possible pathways from the regular expression.\n",
    "def extractPathways(parse_tree):\n",
    "    all_elems = []\n",
    "    mlist = []\n",
    "    #Walks through the parse tree. If a node may have two or more labels\n",
    "    # it is added to our collection as a list of all possible labels.\n",
    "    for child in parse_tree.children:\n",
    "        if type(child) == str:\n",
    "            mlist.append([child])\n",
    "        else:\n",
    "            mlist.append(child)\n",
    "            \n",
    "    #We iterate through all possible combinations of node and edge labels\n",
    "    # along the provided regex.\n",
    "    for i in product(*mlist):\n",
    "        all_elems.append(list(i))\n",
    "        \n",
    "    #Filter null characters out of node labels.\n",
    "    pathways = []\n",
    "    for i in all_elems:\n",
    "        if('' in i): \n",
    "            idx = i.index(\"\")\n",
    "            i.pop(idx)\n",
    "            i.pop(idx-1)\n",
    "            pathways.append(i)\n",
    "        else:\n",
    "            pathways.append(i)\n",
    "    return pathways\n",
    "\n",
    "#Generates a CYPHER query for a regular expression. The first node in the regular expression will be \n",
    "# mapped onto the source_node_name.\n",
    "def getQueries(source_node_name, regexes):\n",
    "    all_queries = []\n",
    "    subgraph_nodes = []\n",
    "    # parse\n",
    "    regex_parser = Lark(regex_grammar, parser='lalr',transformer=CalculateTree())\n",
    "    parsed = regex_parser.parse(regexes)\n",
    "    all_pathways = extractPathways(parsed)\n",
    "    \n",
    "    queryStr = ''\n",
    "    final_idx = len(all_pathways)-1\n",
    "    for i, path in enumerate(all_pathways):\n",
    "        \n",
    "        if(path[-1]==''):path[-1]\n",
    "        start_node_num = path[0].split(\":\")[0].split(\"(\")[1].split(\")\")[0]\n",
    "        query = \"MATCH p1=\"\n",
    "        query += ''.join(str(elem) for elem in path)   \n",
    "        query += \" WHERE %s.name =\" % start_node_num\n",
    "        query += \" '%s'\" % source_node_name\n",
    "        \n",
    "        # add conditions to the node names if repeated types in the path\n",
    "        add_where = \"\"\n",
    "        repeated = []\n",
    "        for j in path:\n",
    "            if \":\" in j: # if type is given\n",
    "                if j.split(\":\")[1] in repeated: # if type is repeated\n",
    "                    if \")\" in j: # if it is a node\n",
    "                        prev = path[repeated.index(j.split(\":\")[1])].split(\":\")[0].split(\"(\")[1]\n",
    "                        current = j.split(\":\")[0].split(\"(\")[1]\n",
    "                        add_where += \" AND %s\" % prev\n",
    "                        add_where += \" <> %s\" % current\n",
    "                repeated.append(j.split(\":\")[1])\n",
    "            else:\n",
    "                repeated.append(\"?\")                \n",
    "            \n",
    "        query += add_where\n",
    "        query += \" WITH collect(p1) as nodez UNWIND nodez as c RETURN c\"\n",
    "        if(i==final_idx): queryStr += query\n",
    "        else: queryStr += query + \" UNION \"\n",
    "    \n",
    "    return queryStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing(regexes):\n",
    "    # parse\n",
    "    regex_parser = Lark(regex_grammar, parser='lalr',transformer=CalculateTree())\n",
    "    reg = regex_parser.parse\n",
    "    parsed = reg(regexes)\n",
    "    all_elems = extractPathways(parsed)\n",
    "\n",
    "    \n",
    "    llink = []\n",
    "    for i in all_elems:\n",
    "        Link = []\n",
    "        for j in i:\n",
    "            if ':' in j:\n",
    "                if '(' in j:\n",
    "                    nodes = j.split(':')[1].split(')')[0]\n",
    "                    Link.append(nodes)\n",
    "                if '[' in j:\n",
    "                    edges = j.split(':')[1].split(']')[0]\n",
    "                    Link.append(edges)\n",
    "            else:\n",
    "                edges = '?'\n",
    "                Link.append(edges)\n",
    "        #print(Link)\n",
    "\n",
    "        for i in range(math.floor(len(Link)/2)):\n",
    "            #print(i)\n",
    "            if i == 0:\n",
    "                llink.append(Link[i:(i+3)])\n",
    "            else:\n",
    "                llink.append(Link[(i*2):(i*2)+3])\n",
    "\n",
    "    return llink\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provide Regex Subgraphs for a Neo4J database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubgraph_neo4j(graph_uri, source_node_name, regexes, compared_labels = None):\n",
    "    \n",
    "    queryStr = getQueries(source_node_name, regexes)    \n",
    "    \n",
    "    driver = GraphDatabase.driver(graph_uri)\n",
    "    \n",
    "    user_labels = []\n",
    "    for ele in parsing(regexes):\n",
    "        user_labels += ele\n",
    "    user_labels = list(set(user_labels))\n",
    "            \n",
    "    with driver.session() as session:\n",
    "        result = session.run(queryStr)\n",
    "        d = {}\n",
    "        join_values = []\n",
    "        for i in result.graph().nodes:\n",
    "            node_name = i['name']\n",
    "            if node_name not in join_values:\n",
    "                #print('labels = ',list(i.labels))\n",
    "                if len(i.labels)>1:\n",
    "                    for m in i.labels:\n",
    "                        if m in user_labels:\n",
    "                            node_type = m\n",
    "                        \n",
    "                        ### for multiple-labeled graph using regex \"? ? ?\"\n",
    "                        \n",
    "                        elif compared_labels != None:\n",
    "                            if m in compared_labels:\n",
    "                                node_type = m\n",
    "                        else:\n",
    "                            node_type = list(i.labels)[0]\n",
    "                        ###\n",
    "                else:\n",
    "                    node_type = list(i.labels)[0]\n",
    "                s = d.get(node_type,set())\n",
    "                s.add(node_name)\n",
    "                d[node_type] = s\n",
    "            join_values.append(node_name)\n",
    "\n",
    "        rels = set()\n",
    "        for i in result.graph().relationships:\n",
    "            start = i.start_node[\"name\"]\n",
    "            end = i.end_node[\"name\"]\n",
    "            rel_type = i.type\n",
    "            rels.add((start, end, rel_type))\n",
    "\n",
    "    raw_nodes = d        \n",
    "    edges = pd.DataFrame.from_records(list(rels),columns=[\"source\",\"target\",\"label\"])\n",
    "\n",
    "    data_frames = {}\n",
    "    for k in d:\n",
    "        node_names = list(d[k])\n",
    "        df = pd.DataFrame({\"name\":node_names}).set_index(\"name\")\n",
    "        data_frames[k] = df\n",
    "\n",
    "    sg = StellarDiGraph(data_frames,edges=edges, edge_type_column=\"label\")\n",
    "   \n",
    "    return sg \n",
    "\n",
    "# find union subgraph for two drugs\n",
    "def querySubgraph(G, regexes, queryStr, compared_labels = None):\n",
    "    \n",
    "    uri = G\n",
    "    driver = GraphDatabase.driver(uri)\n",
    "    \n",
    "    user_labels = []\n",
    "    for ele in parsing(regexes):\n",
    "        user_labels += ele\n",
    "    user_labels = list(set(user_labels))\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        result = session.run(queryStr)\n",
    "        d = {}\n",
    "        join_values = []\n",
    "        for i in result.graph().nodes:\n",
    "            node_name = i['name']\n",
    "            if node_name not in join_values:\n",
    "                #print('labels = ',list(i.labels))\n",
    "                if len(i.labels)>1:\n",
    "                    for m in i.labels:\n",
    "                        if m in user_labels:\n",
    "                            node_type = m\n",
    "                        \n",
    "                        ### for multiple-labeled graph using regex \"? ? ?\"\n",
    "                        \n",
    "                        elif compared_labels != None:\n",
    "                            if m in compared_labels:\n",
    "                                node_type = m\n",
    "                        else:\n",
    "                            node_type = list(i.labels)[0]\n",
    "                        ###\n",
    "                else:\n",
    "                    node_type = list(i.labels)[0]\n",
    "                s = d.get(node_type,set())\n",
    "                s.add(node_name)\n",
    "                d[node_type] = s\n",
    "            join_values.append(node_name)\n",
    "\n",
    "        rels = set()\n",
    "        for i in result.graph().relationships:\n",
    "            start = i.start_node[\"name\"]\n",
    "            end = i.end_node[\"name\"]\n",
    "            rel_type = i.type\n",
    "            rels.add((start, end, rel_type))\n",
    "\n",
    "    raw_nodes = d        \n",
    "    edges = pd.DataFrame.from_records(list(rels),columns=[\"source\",\"target\",\"label\"])\n",
    "\n",
    "    data_frames = {}\n",
    "    for k in d:\n",
    "        node_names = list(d[k])\n",
    "        df = pd.DataFrame({\"name\":node_names}).set_index(\"name\")\n",
    "        data_frames[k] = df\n",
    "\n",
    "    sg = StellarDiGraph(data_frames,edges=edges, edge_type_column=\"label\")\n",
    "    \n",
    "    return sg \n",
    "\n",
    "#Helper function. Constructs a dictonary; the keys are node names provided in\n",
    "# node_list. The values are the semantic subgraphs constructed using the \n",
    "# parameters G, semantic_query, and compare_labels.\n",
    "def buildSubgraphDictonaryForNodes(node_list, G, semantic_query, compared_labels):\n",
    "    subGs = {}\n",
    "    for node in node_list:\n",
    "        subG = getSubgraph_neo4j(G, node, semantic_query, compared_labels)\n",
    "        subGs[node] = subG\n",
    "    return subGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns counts of all node by labels in graph and all relationships by types.\n",
    "def infoDict(subG):\n",
    "    Info = {}\n",
    "    for i in subG.info().split('\\n'):\n",
    "        if '[' in i:\n",
    "            temp = i.split(':')\n",
    "            text = temp[0].strip()\n",
    "            num = temp[1].split('[')[1].split(']')[0]\n",
    "            Info[text] = num\n",
    "        \n",
    "    return Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Random Walks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates random walks for various methods.\n",
    "def compactWalks(subgraph_dict, node_list, method, l, r, metapath = None):\n",
    "    subGs = {}\n",
    "\n",
    "\n",
    "    Walks = []\n",
    "    for node in node_list:\n",
    "        subG = subgraph_dict[node]\n",
    "        # DeepWalk\n",
    "        if method == 'deepwalk':\n",
    "            rw = UniformRandomWalk(subG) #BiasedRandomWalk(G)\n",
    "            walks = rw.run(\n",
    "                nodes= [node],#list(G.nodes()),  # root nodes\n",
    "                length = l,#adj_wlength,  # maximum length of a random walk\n",
    "                n = r #,  # number of random walks per root node\n",
    "                #seed = 1\n",
    "            )\n",
    "\n",
    "        # Node2Vec\n",
    "        elif method == 'node2vec':\n",
    "            rw = BiasedRandomWalk(subG)\n",
    "            walks = rw.run(\n",
    "                nodes= [node],  # root nodes\n",
    "                length = l,  # maximum length of a random walk\n",
    "                n = r,  # number of random walks per root node\n",
    "                p = 0.25,  # Defines (unormalised) probability, 1/p, of returning to source node\n",
    "                q = 0.25#,  # Defines (unormalised) probability, 1/q, for moving away from source node\n",
    "                #seed = 5\n",
    "            )\n",
    "\n",
    "        #Metapath2vec\n",
    "        elif method == 'metapath2vec':\n",
    "            rw = UniformRandomMetaPathWalk(subG)\n",
    "            walks = rw.run(\n",
    "                nodes= [node],#list(G.nodes()),  # root nodes\n",
    "                length = l,  # maximum length of a random walk\n",
    "                n = r,  # number of random walks per root node\n",
    "                metapaths = metapath#,\n",
    "                #seed = 5\n",
    "            )\n",
    "\n",
    "        # append walks\n",
    "        for w in walks:\n",
    "            Walks.append(w)\n",
    "            \n",
    "    return Walks\n",
    "\n",
    "# find semantic ratio for Walks provided.\n",
    "def sematicRatio_walks(regexes, Walks, subGs):\n",
    "    num = 0\n",
    "    den = 0\n",
    "\n",
    "    # parse\n",
    "    llink = parsing(regexes)\n",
    "    print(llink)\n",
    "\n",
    "    # matching process\n",
    "    for i in Walks:     \n",
    "        # matching nodes\n",
    "        for j in i:\n",
    "            res = []\n",
    "            # find node type for j\n",
    "            # if two or more graphs\n",
    "            if type(subGs) == dict: \n",
    "\n",
    "                nodes = []\n",
    "                for n in subGs.keys():\n",
    "                    nodes.append(n)\n",
    "\n",
    "                \n",
    "                for n in nodes:\n",
    "                    if j in subGs[n].nodes():\n",
    "                        node_label = subGs[n].node_type(j)\n",
    "                        break\n",
    "\n",
    "\n",
    "\n",
    "            # if only one graph        \n",
    "            else: \n",
    "                node_label = subGs.node_type(j)\n",
    "\n",
    "            for l in llink:\n",
    "                if node_label in l:\n",
    "                    res.append('Y')\n",
    "                    break\n",
    "                else:\n",
    "                    res.append('N')\n",
    "            \n",
    "            # counting how many signals in nodes\n",
    "            if('Y' in res):\n",
    "                num += 1\n",
    "\n",
    "            den += 1\n",
    "        \n",
    "        \n",
    "        # matching edges\n",
    "        for j in range(len(i)-1):\n",
    "        \n",
    "            res = []\n",
    "\n",
    "            node1 = i[j]\n",
    "            node2 = i[j+1]\n",
    "            \n",
    "            # if two graphs\n",
    "            if type(subGs) == dict: \n",
    "                \n",
    "                for n in nodes:\n",
    "                    if (node1, node2) in subGs[n].edges():\n",
    "                        loc = subGs[n].edges().index((node1, node2))\n",
    "                        edge_label = subGs[n].edges(' ')[loc][2]\n",
    "                        break\n",
    "                    elif (node2, node1) in subGs[n].edges():\n",
    "                        loc = subGs[n].edges().index((node2, node1))\n",
    "                        edge_label = subGs[n].edges(' ')[loc][2]\n",
    "                        break\n",
    "\n",
    "            \n",
    "            # if one graph\n",
    "            else:\n",
    "                if (node1, node2) in subGs.edges():\n",
    "                    loc = subGs.edges().index((node1, node2))\n",
    "                    edge_label = subGs.edges(' ')[loc][2]\n",
    "                elif (node2, node1) in subGs.edges():\n",
    "                    loc = subGs.edges().index((node2, node1))\n",
    "                    edge_label = subGs.edges(' ')[loc][2]\n",
    "            \n",
    "\n",
    "            for l in llink:\n",
    "                if edge_label in l:\n",
    "                    res.append('Y')\n",
    "                    break\n",
    "                else:\n",
    "                    res.append('N')\n",
    "                           \n",
    "            # counting how many signals in edges\n",
    "            if('Y' in res):\n",
    "                num += 1\n",
    "\n",
    "            \n",
    "            den += 1\n",
    "    \n",
    "    print(num, den)\n",
    "    \n",
    "    return round(num/den,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning on Semantic Subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates a model with embeddings from provided collection of Walks.\n",
    "def buildModel(Walks):\n",
    "    str_walks = [[str(n) for n in walk] for walk in Walks]\n",
    "    model = Word2Vec(str_walks, size=128, window=10, min_count=0, sg=1, workers=2, iter=5)\n",
    "    return model\n",
    "\n",
    "#Computes various benchmarks for a machine learning models.\n",
    "def evaluate(model, subgraph_dict, node_list1, node_list2):\n",
    "    evaluate_dict = {}\n",
    "    hit_at_1_in_list = 0\n",
    "    hit_at_3_in_list = 0\n",
    "    hit_at_5_in_list = 0\n",
    "    mrr_in_list = 0\n",
    "    \n",
    "    Node_List = node_list1 + node_list2\n",
    "    for n in node_list1:\n",
    "\n",
    "        print(\"==\", n, \"==\")\n",
    "        n_all = 0\n",
    "        num_in_list = 0\n",
    "        rank_in_list = 0\n",
    "\n",
    "        for i in model.wv.most_similar(n, topn = 20000):\n",
    "            n_all += 1\n",
    "\n",
    "            for j in Node_List:\n",
    "                if i[0] in subgraph_dict[j].nodes():\n",
    "                    nodeType = subgraph_dict[j].node_type(i[0])\n",
    "                    break\n",
    "\n",
    "            if i[0] in Node_List:\n",
    "                num_in_list += 1\n",
    "\n",
    "            if i[0] == node_list2[node_list1.index(n)]:\n",
    "                print('drugs* ',num_in_list)\n",
    "                # test: include only drugs in list\n",
    "                rank_in_list = num_in_list\n",
    "                if rank_in_list == 1:\n",
    "                    hit_at_1_in_list += 1\n",
    "                if rank_in_list <= 3:\n",
    "                    hit_at_3_in_list += 1\n",
    "                if rank_in_list <= 5:\n",
    "                    hit_at_5_in_list += 1\n",
    "                mrr_in_list += 1/rank_in_list\n",
    "\n",
    "    print('compute only in list:')\n",
    "    print(\"num of Compound*: \",num_in_list)\n",
    "    print(\"HIT@1 = \", round(hit_at_1_in_list/len(node_list1),4))\n",
    "    print(\"HIT@3 = \", round(hit_at_3_in_list/len(node_list1),4))\n",
    "    print(\"HIT@5 = \", round(hit_at_5_in_list/len(node_list1),4))\n",
    "    print(\"MRR = \", round(mrr_in_list/len(node_list1),4))\n",
    "    HIT1 = round(hit_at_1_in_list/len(node_list1),4)\n",
    "    HIT3 = round(hit_at_3_in_list/len(node_list1),4)\n",
    "    HIT5 = round(hit_at_5_in_list/len(node_list1),4)\n",
    "    MRR = round(mrr_in_list/len(node_list1),4)\n",
    "    \n",
    "    evaluate_dict['HIT@1'] = HIT1\n",
    "    evaluate_dict['HIT@3'] = HIT3\n",
    "    evaluate_dict['HIT@5'] = HIT5\n",
    "    evaluate_dict['MRR'] = MRR\n",
    "    \n",
    "    return evaluate_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Example Run</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given G, user_input, Node_List, method(m), walk_length(l), num_walk(r), compared_labels, metapath     \n",
    "node_list = ['Canagliflozin', 'Dapagliflozin','Dexamethasone', 'Betamethasone','Lapatinib', 'Afatinib',\n",
    "            'Captopril', 'Enalapril','Losartan', 'Valsartan','Nifedipine', 'Felodipine',\n",
    "            'Simvastatin', 'Atorvastatin','Alendronate', 'Incadronate','Citalopram', 'Escitalopram']\n",
    "\n",
    "semantic_query = \"Compound BINDS_CbG Gene ASSOCIATES_DaG< Disease\"\n",
    "\n",
    "subgraph_dict = buildSubgraphDictonaryForNodes(node_list, \"bolt://neo4j.het.io\", semantic_query, None)\n",
    "\n",
    "Walks = compactWalks(subgraph_dict, node_list, 'deepwalk', 80, 5)\n",
    "model = buildModel(Walks)\n",
    "\n",
    "# Retrieve node embeddings and corresponding subjects\n",
    "node_ids = model.wv.index2word  # list of node IDs\n",
    "node_embeddings = (\n",
    "    model.wv.vectors\n",
    ")\n",
    "\n",
    "evaluate(model, subgraph_dict, ['Canagliflozin', 'Dexamethasone','Lapatinib', \n",
    "            'Captopril','Losartan', 'Nifedipine', \n",
    "            'Simvastatin', 'Alendronate', 'Citalopram'], ['Dapagliflozin','Betamethasone','Afatinib',\n",
    "            'Enalapril','Valsartan','Felodipine',\n",
    "            'Atorvastatin','Incadronate','Escitalopram'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
