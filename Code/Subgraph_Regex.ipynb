{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from itertools import product\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "from lark import Lark, Transformer, v_args\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from stellargraph.data import UniformRandomWalk, BiasedRandomWalk, UniformRandomMetaPathWalk\n",
    "from stellargraph import StellarGraph, StellarDiGraph, datasets\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expression grammar meant for creating succinct patterns through a graph. The grammar below must contain a NODE Identifier, which should correspond to the node label in the Neo4J database. \n",
    "\n",
    "Examples of valid grammar statements:\n",
    "\n",
    "<i>\"chemical_substance treats> disease\"</i> -> this would search for any node with the label <b>chemical_substance</b> connected to nodes of type <b>disease</b> with an edge of type <i>treats</i>.\n",
    "\n",
    "<i>\"chemical_substance ? disease\"</i> -> this would search for any node with the label <b>chemical_substance</b> connected to nodes of type <b>disease</b> with any edge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_grammar = \"\"\"\n",
    "    start: node \n",
    "         | node (edge node)+\n",
    "         | node \"(\" path \")\"\n",
    "\n",
    "       \n",
    "    ?path: edge_node\n",
    "         | path \"|\" edge_node   -> path_or \n",
    "    \n",
    "    ?edge_node: edge node\n",
    "         | \"(\" edge_node+ \")\" \n",
    "         \n",
    "\n",
    "         \n",
    "    ?edge: attm\n",
    "         | edge \"|\" attm        -> edge_or\n",
    "         \n",
    "    ?attm: EDGE_LABEL           -> edge\n",
    "         | attm \">\"             -> edge_right\n",
    "         | attm \"<\"             -> edge_left\n",
    "         | NULL                 -> edge_no_label\n",
    "         | \"(\" edge \")\"     \n",
    "    \n",
    "    ?node: atom\n",
    "        | node \"|\" atom         -> node_or\n",
    "\n",
    "    ?atom: NODE_LABEL           -> node\n",
    "         | NODE_LABEL \"*\"       -> rep_from_0\n",
    "         | NODE_LABEL \"+\"       -> rep_from_1\n",
    "         | NULL                 -> node_no_label\n",
    "         | \"(\" node \")\"\n",
    "\n",
    "    EDGE_LABEL: LABEL_STRING\n",
    "    NODE_LABEL: LABEL_STRING\n",
    "    NULL: \"?\"\n",
    "    LCASE_LETTER: \"a\"..\"z\"\n",
    "    UCASE_LETTER: \"A\"..\"Z\"\n",
    "    DIGIT: \"0\"..\"9\"\n",
    "\n",
    "    LETTER: UCASE_LETTER | LCASE_LETTER | DIGIT | \"_\" | \"-\"\n",
    "    LABEL_STRING: LETTER+ | \"_\" \n",
    "\n",
    "    %import common.CNAME -> NAME\n",
    "    %import common.WS_INLINE\n",
    "    %ignore WS_INLINE\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lark parser. Converts a subgraph regular expression into a CYPHER query. These functions act on different triggers provided by the grammar above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@v_args(inline=True)    # Affects the signatures of the methods\n",
    "class CalculateTree(Transformer):\n",
    "    node_idx = 0\n",
    "    edge_idx = 0\n",
    "        \n",
    "    def node(self, name):\n",
    "        self.node_idx += 1\n",
    "        return \"(n{}:\".format(self.node_idx) + str(name) +\")\"\n",
    "    \n",
    "    def edge(self, name):\n",
    "        self.edge_idx += 1\n",
    "        return \"-[r{}:\".format(self.edge_idx) + str(name) +\"]-\"\n",
    "    \n",
    "    def node_no_label(self, name):\n",
    "        self.node_idx += 1\n",
    "        return \"(n{}\".format(self.node_idx) +\")\"\n",
    "    \n",
    "    def edge_no_label(self, name):\n",
    "        self.edge_idx += 1\n",
    "        return \"-[r{}\".format(self.edge_idx) +\"]-\"\n",
    "    \n",
    "    def edge_node(self, name1, name2):\n",
    "        path = name1 + name2\n",
    "        return path\n",
    "\n",
    "    def edge_right(self, name):\n",
    "        path = ''\n",
    "        path = name \n",
    "        return path + \">\"\n",
    "\n",
    "    def edge_left(self, name):\n",
    "        path = ''\n",
    "        path = name \n",
    "        return \"<\" + path\n",
    "\n",
    "    def rep_from_0(self, name):\n",
    "        self.node_idx += 1\n",
    "        path1 = \"(n{}:\".format(self.node_idx) + str(name) +\")\"\n",
    "        path2 = \"(n{}:\".format(self.node_idx) + str(name) +\")\" + \"--\" + \"(n{}:\".format(self.node_idx+1) + str(name) +\")\"\n",
    "        self.node_idx += 1\n",
    "        return ['', path1, path2]\n",
    "\n",
    "\n",
    "    def rep_from_1(self, name):\n",
    "        self.node_idx += 1\n",
    "        path1 = \"(n{}:\".format(self.node_idx) + str(name) +\")\"\n",
    "        path2 = \"(n{}:\".format(self.node_idx) + str(name) +\")\" + \"--\" + \"(n{}:\".format(self.node_idx+1) + str(name) +\")\"\n",
    "        self.node_idx += 1\n",
    "        return [path1, path2]\n",
    "\n",
    "    def node_or(self, name1, name2):\n",
    "        return [name1, name2]\n",
    "    \n",
    "    def edge_or(self, name1, name2):\n",
    "        return [name1, name2]\n",
    "    \n",
    "    def path_or(self, name1, name2):\n",
    "        return [name1, name2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractPathways(parsed):\n",
    "    all_elems = []\n",
    "    mlist = []\n",
    "    #Walks through the parse tree. If a node may have two or more labels\n",
    "    # it is added to our collection as a list of all possible labels.\n",
    "    for child in parsed.children:\n",
    "        if type(child) == str:\n",
    "            mlist.append([child])\n",
    "        else:\n",
    "            mlist.append(child)\n",
    "            \n",
    "    #We iterate through all possible combinations of node and edge labels\n",
    "    # along the provided regex.\n",
    "    for i in product(*mlist):\n",
    "        all_elems.append(list(i))\n",
    "        \n",
    "    #Filter null characters out of node labels.\n",
    "    new_elems = []\n",
    "    for i in all_elems:\n",
    "        if('' in i): \n",
    "            idx = i.index(\"\")\n",
    "            i.pop(idx)\n",
    "            i.pop(idx-1)\n",
    "            new_elems.append(i)\n",
    "        else:\n",
    "            new_elems.append(i)\n",
    "    return new_elems\n",
    "\n",
    "def getQueries(source_node_name, regexes):\n",
    "    all_queries = []\n",
    "    subgraph_nodes = []\n",
    "    # parse\n",
    "    regex_parser = Lark(regex_grammar, parser='lalr',transformer=CalculateTree())\n",
    "    parsed = regex_parser.parse(regexes)\n",
    "    all_pathways = extractPathways(parsed)\n",
    "    \n",
    "    queryStr = ''\n",
    "    final_idx = len(all_pathways)-1\n",
    "    for i, path in enumerate(all_pathways):\n",
    "        \n",
    "        if(path[-1]==''):path[-1]\n",
    "        start_node_num = path[0].split(\":\")[0].split(\"(\")[1].split(\")\")[0]\n",
    "        query = \"MATCH p1=\"\n",
    "        query += ''.join(str(elem) for elem in path)   \n",
    "        query += \" WHERE %s.name =\" % start_node_num\n",
    "        query += \" '%s'\" % source_node_name\n",
    "        \n",
    "        # add conditions to the node names if repeated types in the path\n",
    "        add_where = \"\"\n",
    "        repeated = []\n",
    "        for j in path:\n",
    "            if \":\" in j: # if type is given\n",
    "                if j.split(\":\")[1] in repeated: # if type is repeated\n",
    "                    if \")\" in j: # if it is a node\n",
    "                        prev = path[repeated.index(j.split(\":\")[1])].split(\":\")[0].split(\"(\")[1]\n",
    "                        current = j.split(\":\")[0].split(\"(\")[1]\n",
    "                        add_where += \" AND %s\" % prev\n",
    "                        add_where += \" <> %s\" % current\n",
    "                repeated.append(j.split(\":\")[1])\n",
    "            else:\n",
    "                repeated.append(\"?\")                \n",
    "            \n",
    "        query += add_where\n",
    "        query += \" WITH collect(p1) as nodez UNWIND nodez as c RETURN c\"\n",
    "        if(i==final_idx): queryStr += query\n",
    "        else: queryStr += query + \" UNION \"\n",
    "    \n",
    "    return queryStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing(regexes):\n",
    "    # parse\n",
    "    regex_parser = Lark(regex_grammar, parser='lalr',transformer=CalculateTree())\n",
    "    reg = regex_parser.parse\n",
    "    parsed = reg(regexes)\n",
    "    all_elems = extractPathways(parsed)\n",
    "\n",
    "    \n",
    "    llink = []\n",
    "    for i in all_elems:\n",
    "        Link = []\n",
    "        for j in i:\n",
    "            if ':' in j:\n",
    "                if '(' in j:\n",
    "                    nodes = j.split(':')[1].split(')')[0]\n",
    "                    Link.append(nodes)\n",
    "                if '[' in j:\n",
    "                    edges = j.split(':')[1].split(']')[0]\n",
    "                    Link.append(edges)\n",
    "            else:\n",
    "                edges = '?'\n",
    "                Link.append(edges)\n",
    "        #print(Link)\n",
    "\n",
    "        for i in range(math.floor(len(Link)/2)):\n",
    "            #print(i)\n",
    "            if i == 0:\n",
    "                llink.append(Link[i:(i+3)])\n",
    "            else:\n",
    "                llink.append(Link[(i*2):(i*2)+3])\n",
    "\n",
    "    return llink\n",
    "\n",
    "\n",
    "def getSubgraph_neo4j(uri, source_node_name, regexes, compared_labels = None):\n",
    "    \n",
    "    queryStr = getQueries(source_node_name, regexes)    \n",
    "    \n",
    "    driver = GraphDatabase.driver(uri)\n",
    "    \n",
    "    user_labels = []\n",
    "    for ele in parsing(regexes):\n",
    "        user_labels += ele\n",
    "    user_labels = list(set(user_labels))\n",
    "            \n",
    "    with driver.session() as session:\n",
    "        result = session.run(queryStr)\n",
    "        d = {}\n",
    "        join_values = []\n",
    "        for i in result.graph().nodes:\n",
    "            node_name = i['name']\n",
    "            if node_name not in join_values:\n",
    "                #print('labels = ',list(i.labels))\n",
    "                if len(i.labels)>1:\n",
    "                    for m in i.labels:\n",
    "                        if m in user_labels:\n",
    "                            node_type = m\n",
    "                        \n",
    "                        ### for multiple-labeled graph using regex \"? ? ?\"\n",
    "                        \n",
    "                        elif compared_labels != None:\n",
    "                            if m in compared_labels:\n",
    "                                node_type = m\n",
    "                        else:\n",
    "                            node_type = list(i.labels)[0]\n",
    "                        ###\n",
    "                else:\n",
    "                    node_type = list(i.labels)[0]\n",
    "                s = d.get(node_type,set())\n",
    "                s.add(node_name)\n",
    "                d[node_type] = s\n",
    "            join_values.append(node_name)\n",
    "\n",
    "        rels = set()\n",
    "        for i in result.graph().relationships:\n",
    "            start = i.start_node[\"name\"]\n",
    "            end = i.end_node[\"name\"]\n",
    "            rel_type = i.type\n",
    "            rels.add((start, end, rel_type))\n",
    "\n",
    "    raw_nodes = d        \n",
    "    edges = pd.DataFrame.from_records(list(rels),columns=[\"source\",\"target\",\"label\"])\n",
    "\n",
    "    data_frames = {}\n",
    "    for k in d:\n",
    "        node_names = list(d[k])\n",
    "        df = pd.DataFrame({\"name\":node_names}).set_index(\"name\")\n",
    "        data_frames[k] = df\n",
    "\n",
    "    sg = StellarDiGraph(data_frames,edges=edges, edge_type_column=\"label\")\n",
    "\n",
    "    #print(sg.info())    \n",
    "    return sg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns counts of all node by labels in graph and all relationships by types.\n",
    "def infoDict(subG):\n",
    "    Info = {}\n",
    "    for i in subG.info().split('\\n'):\n",
    "        if '[' in i:\n",
    "            temp = i.split(':')\n",
    "            text = temp[0].strip()\n",
    "            num = temp[1].split('[')[1].split(']')[0]\n",
    "            Info[text] = num\n",
    "        \n",
    "    return Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find union subgraph for two drugs\n",
    "\n",
    "def querySubgraph(G, regexes, queryStr, compared_labels = None):\n",
    "    \n",
    "    uri = G\n",
    "    driver = GraphDatabase.driver(uri)\n",
    "    \n",
    "    user_labels = []\n",
    "    for ele in parsing(regexes):\n",
    "        user_labels += ele\n",
    "    user_labels = list(set(user_labels))\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        result = session.run(queryStr)\n",
    "        d = {}\n",
    "        join_values = []\n",
    "        for i in result.graph().nodes:\n",
    "            node_name = i['name']\n",
    "            if node_name not in join_values:\n",
    "                #print('labels = ',list(i.labels))\n",
    "                if len(i.labels)>1:\n",
    "                    for m in i.labels:\n",
    "                        if m in user_labels:\n",
    "                            node_type = m\n",
    "                        \n",
    "                        ### for multiple-labeled graph using regex \"? ? ?\"\n",
    "                        \n",
    "                        elif compared_labels != None:\n",
    "                            if m in compared_labels:\n",
    "                                node_type = m\n",
    "                        else:\n",
    "                            node_type = list(i.labels)[0]\n",
    "                        ###\n",
    "                else:\n",
    "                    node_type = list(i.labels)[0]\n",
    "                s = d.get(node_type,set())\n",
    "                s.add(node_name)\n",
    "                d[node_type] = s\n",
    "            join_values.append(node_name)\n",
    "\n",
    "        rels = set()\n",
    "        for i in result.graph().relationships:\n",
    "            start = i.start_node[\"name\"]\n",
    "            end = i.end_node[\"name\"]\n",
    "            rel_type = i.type\n",
    "            rels.add((start, end, rel_type))\n",
    "\n",
    "    raw_nodes = d        \n",
    "    edges = pd.DataFrame.from_records(list(rels),columns=[\"source\",\"target\",\"label\"])\n",
    "\n",
    "    data_frames = {}\n",
    "    for k in d:\n",
    "        node_names = list(d[k])\n",
    "        df = pd.DataFrame({\"name\":node_names}).set_index(\"name\")\n",
    "        data_frames[k] = df\n",
    "\n",
    "    sg = StellarDiGraph(data_frames,edges=edges, edge_type_column=\"label\")\n",
    "    \n",
    "    return sg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find semantic ratio for walks\n",
    "        \n",
    "def semanticRatio_walks(regexes, Walks, subGs):\n",
    "    num = 0\n",
    "    den = 0\n",
    "\n",
    "    # parse\n",
    "    llink = parsing(regexes)\n",
    "    print(llink)\n",
    "\n",
    "    # matching process\n",
    "    for i in Walks:     \n",
    "        # matching nodes\n",
    "        for j in i:\n",
    "            res = []\n",
    "            # find node type for j\n",
    "            # if two graphs\n",
    "            if type(subGs) == dict: \n",
    "\n",
    "                nodes = []\n",
    "                for n in subGs.keys():\n",
    "                    nodes.append(n)\n",
    "                    \n",
    "                n1 = str(nodes[0])\n",
    "                n2 = str(nodes[1])\n",
    "\n",
    "                if j in subGs[n1].nodes():\n",
    "                    node_label = subGs[n1].node_type(j)\n",
    "                elif j in subGs[n2].nodes():\n",
    "                    node_label = subGs[n2].node_type(j)\n",
    "                else:\n",
    "                    print('WEIRD. Node not in subG1 and subG2.')\n",
    "\n",
    "\n",
    "            # if only one graph        \n",
    "            else: \n",
    "                node_label = subGs.node_type(j)\n",
    "\n",
    "       \n",
    "            for l in llink:\n",
    "                if node_label in l:\n",
    "                    res.append('Y')\n",
    "                    break\n",
    "                else:\n",
    "                    res.append('N')\n",
    "            \n",
    "            # counting how many signals in nodes\n",
    "            if('Y' in res):\n",
    "                #print(j, l, 'signal')\n",
    "                num += 1\n",
    "            #else:\n",
    "                #print(j, l, 'noise')\n",
    "            den += 1\n",
    "        \n",
    "        \n",
    "        # matching edges\n",
    "        for j in range(len(i)-1):\n",
    "        \n",
    "            res = []\n",
    "\n",
    "            node1 = i[j]\n",
    "            node2 = i[j+1]\n",
    "            \n",
    "            # if two graphs\n",
    "            if type(subGs) == dict: \n",
    "                if (node1, node2) in subGs[n1].edges():\n",
    "                    loc = subGs[n1].edges().index((node1, node2))\n",
    "                    edge_label = subGs[n1].edges(' ')[loc][2]\n",
    "                elif (node2, node1) in subGs[n1].edges():\n",
    "                    loc = subGs[n1].edges().index((node2, node1))\n",
    "                    edge_label = subGs[n1].edges(' ')[loc][2]\n",
    "                elif (node1, node2) in subGs[n2].edges():\n",
    "                    loc = subGs[n2].edges().index((node1, node2))\n",
    "                    edge_label = subGs[n2].edges(' ')[loc][2]\n",
    "                elif (node2, node1) in subGs[n2].edges():\n",
    "                    loc = subGs[n2].edges().index((node2, node1))\n",
    "                    edge_label = subGs[n2].edges(' ')[loc][2]\n",
    "                else:\n",
    "                    print(\"WEIRD. Edge not in subG1 and subG2.\")\n",
    "            \n",
    "            # if one graph\n",
    "            else:\n",
    "                if (node1, node2) in subGs.edges():\n",
    "                    loc = subGs.edges().index((node1, node2))\n",
    "                    edge_label = subGs.edges(' ')[loc][2]\n",
    "                elif (node2, node1) in subGs.edges():\n",
    "                    loc = subGs.edges().index((node2, node1))\n",
    "                    edge_label = subGs.edges(' ')[loc][2]\n",
    "            \n",
    "\n",
    "            for l in llink:\n",
    "                if edge_label in l:\n",
    "                    res.append('Y')\n",
    "                    break\n",
    "                else:\n",
    "                    res.append('N')\n",
    "                           \n",
    "            if('Y' in res):\n",
    "                num += 1\n",
    "            \n",
    "            den += 1\n",
    "    \n",
    "    print(num, den)\n",
    "    \n",
    "    return round(num/den,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subGs = {}\n",
    "SRdict = {}\n",
    "for i in node_list:\n",
    "    print('building subgraph for ',i)\n",
    "    subG = getSubgraph_neo4j(G, i, user_input)\n",
    "    subGs[i] = subG\n",
    "    SR = semanticRatio(user_input, subG)\n",
    "    SRdict[i] = SR\n",
    "subGs\n",
    "SRdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Random Walks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "G = \"bolt://robokopkg.renci.org\"\n",
    "user_input = \"chemical_substance decreases_activity_of> gene causes> disease\"\n",
    "compared_labels = ['chemical_substance', 'gene', 'disease']\n",
    "\n",
    "\n",
    "\n",
    "Node_List = [['dexamethasone', 'canagliflozin'],\n",
    "['afatinib', 'captopril'],\n",
    "\n",
    "['escitalopram', 'losartan'],\n",
    "['betamethasone', 'enalapril'],\n",
    "['dapagliflozin', 'nifedipine'],\n",
    "['escitalopram', 'felodipine']]\n",
    "#['simvastatin', 'alendronate sodium trihydrate'],\n",
    "    \n",
    "def buildAndWalkSubgraphs(G, node_list, user_input, compared_labels, walk_lengths=[10, 20, 40, 60, 80])\n",
    "    Results =[]\n",
    "    subGs = {}\n",
    "    SRdict = {}\n",
    "    for node in node_list:\n",
    "        print('building subgraph for ',node)\n",
    "        subG = getSubgraph_neo4j(G, node, user_input, compared_labels)\n",
    "        subGs[node] = subG\n",
    "        SR = semanticRatio(user_input, subG)\n",
    "        SRdict[node] = SR\n",
    "\n",
    "        \n",
    "    cosineSim = []\n",
    "    \n",
    "    for l in walk_lengths\n",
    "        print('** walk length = ', l, '**')\n",
    "\n",
    "        Walks = []\n",
    "        nodeDict = {}\n",
    "        queryString = \"\"\n",
    "\n",
    "        for node in node_list:\n",
    "            print('===',node, '===')\n",
    "            start = time.time()\n",
    "            subG = subGs[node]\n",
    "\n",
    "            # save the UNION query string\n",
    "            query = getQueries(node, user_input)\n",
    "            if(node_list.index(node)==len(node_list)-1): queryString += query\n",
    "            else: queryString += query + \" UNION \"\n",
    "\n",
    "            graph_size = len(list(subG.nodes()))\n",
    "            adj_l = min(l, graph_size)\n",
    "            print('walk length = ', adj_l)\n",
    "\n",
    "            n1 = node\n",
    "\n",
    "            # DeepWalk\n",
    "            rw = UniformRandomWalk(subG) #BiasedRandomWalk(G)\n",
    "            walks = rw.run(\n",
    "                nodes= [n1],#list(G.nodes()),  # root nodes\n",
    "                length = l,#adj_wlength,  # maximum length of a random walk\n",
    "                n = 5 #,  # number of random walks per root node\n",
    "                #seed = 1\n",
    "            )\n",
    "            print(\"DeepWalk:\")\n",
    "            print(\"Number of random walks: {}\".format(len(walks)))\n",
    "            print(len(walks[0]))\n",
    "\n",
    "            # Node2Vec\n",
    "            # append walks\n",
    "            for w in walks:\n",
    "                Walks.append(w)\n",
    "\n",
    "                for node_id in w:\n",
    "                    if len(w)==1:\n",
    "                        nodeDict[node_id] = subG.node_type(n1) \n",
    "                    else:\n",
    "                        nodeDict[node_id] = subG.node_type(node_id) \n",
    "\n",
    "            # count the time\n",
    "            end = time.time()\n",
    "            print(\"Time spend: \", end - start)\n",
    "\n",
    "        # Generate embeddings\n",
    "        from gensim.models import Word2Vec\n",
    "        str_walks = [[str(n) for n in walk] for walk in Walks]\n",
    "        model = Word2Vec(str_walks, size=128, window=5, min_count=0, sg=1, workers=2, iter=5)\n",
    "\n",
    "\n",
    "        # Retrieve node embeddings and corresponding subjects\n",
    "        node_ids = model.wv.index2word  # list of node IDs\n",
    "        node_embeddings = (\n",
    "            model.wv.vectors\n",
    "        )\n",
    "        # Check similarity from embedding vectors\n",
    "        n2 = node_list[0]\n",
    "        n3 = node_list[1]\n",
    "        print(n2, n3)\n",
    "        print(node_ids.index(n2))\n",
    "        print(node_ids.index(n3))\n",
    "        n2_embeddings = node_embeddings[node_ids.index(n2)]\n",
    "        n3_embeddings = node_embeddings[node_ids.index(n3)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        sim = cosine_similarity([n2_embeddings], [n3_embeddings])\n",
    "        cosineSim.append(sim)\n",
    "\n",
    "\n",
    "        # semantic ratio for walks\n",
    "        SR_walks = semanticRatio_walks(user_input, Walks, subGs)\n",
    "        print(SR_walks)\n",
    "\n",
    "        # check rank for all nodes\n",
    "        num = 0\n",
    "        rank = 0\n",
    "        for i in model.wv.most_similar(n2,topn=2000):\n",
    "            num += 1\n",
    "            if i[0] == n3:\n",
    "                print(num, i)\n",
    "                rank = num\n",
    "\n",
    "        print('rank = ', rank,\"/\",num)\n",
    "        prop = round(rank/num, 4)\n",
    "\n",
    "        # rank include only Compound\n",
    "        num_ex = 0\n",
    "        n_all = 0\n",
    "        rank_ex = 0\n",
    "        for i in model.wv.most_similar(n2, topn = 2000):\n",
    "            n_all += 1\n",
    "            if i[0] in subGs[n2].nodes():\n",
    "                nodeType = subGs[n2].node_type(i[0])\n",
    "            else:\n",
    "                nodeType = subGs[n3].node_type(i[0])\n",
    "\n",
    "            if nodeType == subGs[n2].node_type(n2):\n",
    "                num_ex += 1\n",
    "                if i[0] == n3:\n",
    "                    print(num_ex, i)\n",
    "                    rank_ex = num_ex\n",
    "\n",
    "        print('rank exclude not Compound = ', rank_ex, '/', num_ex)\n",
    "        print('number of drugs', num_ex)\n",
    "        print('number of nodes', n_all)\n",
    "\n",
    "        print(node_list, SRdict, l, sim.tolist()[0][0], rank, num, prop, rank_ex, num_ex, SR_walks)\n",
    "        result_row = [node_list, SRdict, l, sim.tolist()[0][0], rank, num, prop, rank_ex, num_ex, SR_walks]\n",
    "        Results.append(result_row)\n",
    "    return Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(Results, columns=['drug_pairs', 'SR_subgraphs', 'walk_length', 'cos_sim', 'rank', 'num_nodes', 'prop', 'rank_ex', 'num_nodes_ex', 'SR_walks'])\n",
    "df.to_csv('robokop_neg_semantic.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check common nodes in subgraphs\n",
    "\n",
    "\n",
    "#node_list = ['Canagliflozin', 'Dapagliflozin'] #0.31\n",
    "#node_list = ['Dexamethasone', 'Betamethasone'] #0.479\n",
    "#node_list = ['Lapatinib', 'Afatinib'] #0.156\n",
    "#node_list = ['Captopril', 'Enalapril'] #0.346\n",
    "#node_list = ['Losartan', 'Valsartan'] #0.322\n",
    "#node_list = ['Nifedipine', 'Felodipine'] #0.305\n",
    "#node_list = ['Simvastatin', 'Atorvastatin'] #0.216\n",
    "#node_list = ['Alendronate', 'Incadronate'] #0.041 @@\n",
    "#node_list = ['Citalopram', 'Escitalopram'] #0.412\n",
    "\n",
    "# negative\n",
    "#node_list =['Dexamethasone', 'Voglibose'] # 0.006\n",
    "#node_list =['Lapatinib', 'Voglibose'] # 0\n",
    "\n",
    "#node_list =['Dexamethasone', 'Canagliflozin'] # 0.118\n",
    "#node_list = ['Afatinib', 'Captopril'] # 0.168\n",
    "#node_list = ['Simvastatin', 'Alendronate'] # 0.104\n",
    "#node_list = ['Escitalopram', 'Losartan'] # 0.175\n",
    "#node_list = ['Betamethasone', 'Enalapril'] # 0.21\n",
    "#node_list = ['Dapagliflozin', 'Nifedipine'] # 0.294\n",
    "#node_list = ['Atorvastatin', 'Incadronate'] # 0.012\n",
    "node_list = ['Citalopram', 'Felodipine'] # 0.284\n",
    "\n",
    "\n",
    "\n",
    "#'Methylphenobarbital', 'Talbutal', 'Amobarbital', 'Etidronic acid'\n",
    "list_compare = []\n",
    "for i in node_list:\n",
    "    \n",
    "    subG = getSubgraph(G, i, user_input)\n",
    "    list_subG = list(subG.nodes())\n",
    "    print('# total nodes = ', len(list_subG))\n",
    "    list_compare.append(list_subG)\n",
    "\n",
    "list_u = set(list_compare[0]).intersection(set(list_compare[1]))\n",
    "print(len(list_u))\n",
    "print(len(list_compare[0])+len(list_compare[1]))\n",
    "ratio = len(list_u)/(len(list_compare[0])+len(list_compare[1]))\n",
    "print(ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_input = \"Compound ? (Gene|Disease) ASSOCIATES_DaG< Disease\"\n",
    "Node_List = [['Dexamethasone', 'Betamethasone'],['Lapatinib', 'Afatinib'],\n",
    "            ['Captopril', 'Enalapril'],['Losartan', 'Valsartan'],['Nifedipine', 'Felodipine'],\n",
    "            ['Simvastatin', 'Atorvastatin'],['Citalopram', 'Escitalopram']]\n",
    "\n",
    "#['Canagliflozin', 'Dapagliflozin'],['Alendronate', 'Incadronate'],\n",
    "# Node_List = [\n",
    "#             ['Afatinib', 'Captopril'],['Simvastatin', 'Alendronate'],['Escitalopram', 'Losartan'],\n",
    "#             ['Betamethasone', 'Enalapril'],['Dapagliflozin', 'Nifedipine'],\n",
    "#             ['Citalopram', 'Felodipine']]\n",
    "\n",
    "#['Dexamethasone', 'Voglibose'],['Lapatinib', 'Voglibose'],['Atorvastatin', 'Incadronate'],['Dexamethasone', 'Canagliflozin'],\n",
    "\n",
    "# Node_List = [['dexamethasone', 'betamethasone'],\n",
    "#  ['canagliflozin', 'dapagliflozin'],\n",
    "#  ['lapatinib', 'afatinib'],\n",
    "#  ['captopril', 'enalapril'],\n",
    "#  ['losartan', 'valsartan'],\n",
    "#  ['nifedipine', 'felodipine'],\n",
    "#  ['simvastatin', 'atorvastatin'],\n",
    "# ['fluconazole', 'voriconazole']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compared_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results = []\n",
    "walk_length = [10, 20, 40, 60, 80]\n",
    "\n",
    "for node_list in Node_List:\n",
    "    subGs = {}\n",
    "    SRdict = {}\n",
    "    for i in node_list:\n",
    "        print('building subgraph for ',i)\n",
    "        query = \"Match (n0)-[r1]->(n1)-[r2]->(n2) Where n0.name = '%s' AND NOT n1:gene AND NOT n2:gene Return * limit 1000\"% i\n",
    "        \n",
    "        #query = \"Match (n0)-[r1]->(n1)-[r2]->(n2) Where n0.name = '%s' Return * limit 1000\"% i\n",
    "        print(query)\n",
    "        subG = querySubgraph(G, user_input, query, compared_labels)#getSubgraph_neo4j(G, i, user_input)\n",
    "        subGs[i] = subG\n",
    "        SR = semamticRatio(user_input, subG)\n",
    "        SRdict[i] = SR\n",
    "\n",
    "    cosineSim = []\n",
    "    for l in walk_length:\n",
    "        print('** walk length = ', l, '**')\n",
    "        Walks = []\n",
    "        nodeDict = {}\n",
    "        for i in node_list:\n",
    "            print('===',i, '===')\n",
    "            subG = subGs[i]\n",
    "            \n",
    "            # appropriate size of walk length (Mar 14)\n",
    "            graph_size = len(list(subG.nodes()))\n",
    "            adj_l = min(l, graph_size)\n",
    "            print('walk length = ', adj_l)\n",
    "\n",
    "\n",
    "            # appropriate size of walk length (Mar 14)\n",
    "            #graph_size = len(list(subG.nodes()))\n",
    "            #adj_wlength = min(l, round(2 * graph_size))\n",
    "\n",
    "\n",
    "            # DeepWalk\n",
    "            rw = UniformRandomWalk(subG) #BiasedRandomWalk(G)\n",
    "            walks = rw.run(\n",
    "                nodes= [i], #list(G.nodes()),  # root nodes\n",
    "                length = l,#adj_wlength,  # maximum length of a random walk\n",
    "                n = 5 #,  # number of random walks per root node\n",
    "                #seed = 1\n",
    "            )\n",
    "            print(\"DeepWalk:\")\n",
    "            print(\"Number of random walks: {}\".format(len(walks)))\n",
    "            #print(len(walks[0]))\n",
    "\n",
    "            # Node2Vec\n",
    "    #         rw = BiasedRandomWalk(subG)\n",
    "    #         walks = rw.run(\n",
    "    #             nodes= [i],  # root nodes\n",
    "    #             length = adj_l,  # maximum length of a random walk\n",
    "    #             n = 5,  # number of random walks per root node\n",
    "    #             p = 2,  # Defines (unormalised) probability, 1/p, of returning to source node\n",
    "    #             q = 0.5#,  # Defines (unormalised) probability, 1/q, for moving away from source node\n",
    "    #             #seed = 5\n",
    "    #         )\n",
    "\n",
    "    #         print(\"DeepWalk:\")\n",
    "    #         print(\"Number of random walks: {}\".format(len(walks)))\n",
    "    #         print(len(walks[0]))\n",
    "    #         #print(walks)\n",
    "\n",
    "\n",
    "    #         # Metapath2vec\n",
    "    #         metapath = [['Compound','Gene','Disease', 'Gene', 'Compound']]#,['Compound','Disease','Compound']]\n",
    "    #         rw = UniformRandomMetaPathWalk(subG)\n",
    "\n",
    "    #         walks = rw.run(\n",
    "    #             nodes= [i],#list(G.nodes()),  # root nodes\n",
    "    #             length = l,  # maximum length of a random walk\n",
    "    #             n = 5,  # number of random walks per root node\n",
    "    #             metapaths = metapath#,\n",
    "    #             #seed = 5\n",
    "    #         )\n",
    "    #         print(\"Metapath2vec:\")\n",
    "    #         print(\"Number of random walks: {}\".format(len(walks)))\n",
    "    #         print(len(walks[0]))\n",
    "    #         #print(walks)\n",
    "\n",
    "\n",
    "            # append walks\n",
    "            for w in walks:\n",
    "                Walks.append(w)\n",
    "\n",
    "                for node_id in w:\n",
    "                    if len(w)==1:\n",
    "                        nodeDict[node_id] = subG.node_type(n1) \n",
    "                    else:\n",
    "                        nodeDict[node_id] = subG.node_type(node_id) \n",
    "\n",
    "\n",
    "        # Generate embeddings\n",
    "\n",
    "        str_walks = [[str(n) for n in walk] for walk in Walks]\n",
    "        model = Word2Vec(str_walks, size=128, window=5, min_count=0, sg=1, workers=2, iter=5)\n",
    "\n",
    "\n",
    "        # Retrieve node embeddings and corresponding subjects\n",
    "        node_ids = model.wv.index2word  # list of node IDs\n",
    "        node_embeddings = (\n",
    "            model.wv.vectors\n",
    "        )\n",
    "        # Check similarity from embedding vectors\n",
    "        n2 = node_list[0]\n",
    "        n3 = node_list[1]\n",
    "        print(n2, n3)\n",
    "        print(node_ids.index(n2))\n",
    "        print(node_ids.index(n3))\n",
    "        n2_embeddings = node_embeddings[node_ids.index(n2)]\n",
    "        n3_embeddings = node_embeddings[node_ids.index(n3)]\n",
    "\n",
    "        sim = cosine_similarity([n2_embeddings], [n3_embeddings])\n",
    "        cosineSim.append(sim)\n",
    "\n",
    "\n",
    "        # semantic ratio for walks\n",
    "        SR_walks = semanticRatio_walks(user_input, Walks, subGs)\n",
    "        print(SR_walks)\n",
    "\n",
    "        # check rank for all nodes\n",
    "        num = 0\n",
    "        rank = 0\n",
    "        for i in model.wv.most_similar(n2,topn=2000):\n",
    "            num += 1\n",
    "            if i[0] == n3:\n",
    "                print(num, i)\n",
    "                rank = num\n",
    "\n",
    "        print('rank = ', rank,\"/\",num)\n",
    "        prop = round(rank/num, 4)\n",
    "\n",
    "        # rank include only Compound\n",
    "        num_ex = 0\n",
    "        n_all = 0\n",
    "        rank_ex = 0\n",
    "        for i in model.wv.most_similar(n2, topn = 2000):\n",
    "            n_all += 1\n",
    "            #print(i)\n",
    "            if i[0] in subGs[n2].nodes():\n",
    "                #print('in n2')\n",
    "                nodeType = subGs[n2].node_type(i[0])\n",
    "            elif i[0] in subGs[n3].nodes():\n",
    "                #print('in n3')\n",
    "                nodeType = subGs[n3].node_type(i[0])\n",
    "            else:\n",
    "                print('Not in n2 and n3')\n",
    "                nodeType = 'none'\n",
    "\n",
    "            if nodeType == subGs[n2].node_type(n2):\n",
    "                num_ex += 1\n",
    "                if i[0] == n3:\n",
    "                    print(num_ex, i)\n",
    "                    rank_ex = num_ex\n",
    "\n",
    "        print('rank exclude not Compound = ', rank_ex, '/', num_ex)\n",
    "        print('number of drugs', num_ex)\n",
    "        print('number of nodes', n_all)\n",
    "\n",
    "        print(node_list, SRdict, l, sim.tolist()[0][0], rank, num, prop, rank_ex, num_ex, SR_walks)\n",
    "        result_row = [node_list, SRdict, l, sim.tolist()[0][0], rank, num, prop, rank_ex, num_ex, SR_walks]\n",
    "        Results.append(result_row)\n",
    "\n",
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(Results, columns=['drug_pairs', 'SR_subgraphs', 'walk_length', 'cos_sim', 'rank', 'num_nodes', 'prop', 'rank_ex', 'num_nodes_ex', 'SR_walks'])\n",
    "df.to_csv('robokop_neg_nonsemantic_lower.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic ratio for walks (not include edges)\n",
    "semanticRatio_walks(user_input, Walks, subGs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(Counter(nodeDict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subG.node_type(n3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive pairs\n",
    "#node_list = ['Canagliflozin', 'Dapagliflozin'] #0.271\n",
    "#node_list = ['Dexamethasone', 'Betamethasone'] #0.0981\n",
    "#node_list = ['Lapatinib', 'Afatinib'] #0.0903\n",
    "#node_list = ['Captopril', 'Enalapril'] #0.1106\n",
    "node_list = ['Losartan', 'Valsartan'] #?\n",
    "#node_list = ['Nifedipine', 'Felodipine'] #0.0628\n",
    "#node_list = ['Simvastatin', 'Atorvastatin'] #0.0467\n",
    "#node_list = ['Alendronate', 'Incadronate'] #0.1075\n",
    "#node_list = ['Citalopram', 'Escitalopram'] #0.3623\n",
    "\n",
    "# negative pairs\n",
    "#node_list =['Dexamethasone', 'Voglibose'] # 0.0508\n",
    "#node_list =['Lapatinib', 'Voglibose'] # 0.0447\n",
    "\n",
    "#node_list =['Dexamethasone', 'Canagliflozin'] # 0.066\n",
    "#node_list = ['Afatinib', 'Captopril'] # 0.104\n",
    "#node_list = ['Simvastatin', 'Alendronate'] # 0.105\n",
    "#node_list = ['Escitalopram', 'Losartan'] # 0.0807\n",
    "#node_list = ['Betamethasone', 'Enalapril'] # 0.078\n",
    "#node_list = ['Dapagliflozin', 'Nifedipine'] # 0.0775\n",
    "#node_list = ['Atorvastatin', 'Incadronate'] # 0.0194\n",
    "#node_list = ['Citalopram', 'Felodipine'] # 0.0913\n",
    "\n",
    "\n",
    "list_compare = []\n",
    "for i in node_list:\n",
    "    #list_subG = []\n",
    "    \n",
    "    #query = \"Match (n0)-[r1]->(n1)-[r2]->(n2)-[r3]-(n3) Where n0.name = '%s' Return * limit 200\"% i\n",
    "    query = \"Match (n0)-[r1]->(n1)-[r2]->(n2) Where n0.name = '%s' Return * limit 1000\"% i\n",
    "    \n",
    "    print(query)\n",
    "    subG = getSubgraph_non(G, i, query)\n",
    "    \n",
    "    \n",
    "    list_subG = list(subG.nodes())\n",
    "    print('# total nodes = ', len(list_subG))\n",
    "    list_compare.append(list_subG)\n",
    "\n",
    "\n",
    "list_u = set(list_compare[0]).intersection(set(list_compare[1]))\n",
    "print('# common nodes = ',len(list_u))\n",
    "print(len(list_compare[0])+len(list_compare[1]))\n",
    "ratio = len(list_u)/(len(list_compare[0])+len(list_compare[1]))\n",
    "print(ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Examples</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "compound = 'Simvastatin'\n",
    "example_expression = \"Compound ? (Gene|Disease) ASSOCIATES_DaG< Disease\"\n",
    "sg = getSubgraph_neo4j('bolt://neo4j.het.io/', compound, example_expression)\n",
    "end = time.time()\n",
    "print(sg.info())\n",
    "print(\"Time spent: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infoDict(sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
