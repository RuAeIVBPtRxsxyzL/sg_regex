{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from itertools import product\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "from lark import Lark, Transformer, v_args\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from stellargraph.data import UniformRandomWalk, BiasedRandomWalk, UniformRandomMetaPathWalk\n",
    "from stellargraph import StellarGraph, StellarDiGraph, datasets\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from sklearn import cluster, datasets, metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_grammar = \"\"\"\n",
    "    start: node \n",
    "         | node (edge node)+\n",
    "         | node \"(\" path \")\"\n",
    "\n",
    "       \n",
    "    ?path: edge_node\n",
    "         | path \"|\" edge_node   -> path_or \n",
    "    \n",
    "    ?edge_node: edge node\n",
    "         | \"(\" edge_node+ \")\" \n",
    "         \n",
    "\n",
    "         \n",
    "    ?edge: attm\n",
    "         | edge \"|\" attm        -> edge_or\n",
    "         \n",
    "    ?attm: EDGE_LABEL           -> edge\n",
    "         | attm \">\"             -> edge_right\n",
    "         | attm \"<\"             -> edge_left\n",
    "         | NULL                 -> edge_no_label\n",
    "         | \"(\" edge \")\"     \n",
    "    \n",
    "    ?node: atom\n",
    "        | node \"|\" atom         -> node_or\n",
    "\n",
    "    ?atom: NODE_LABEL           -> node\n",
    "         | NODE_LABEL \"*\"       -> rep_from_0\n",
    "         | NODE_LABEL \"+\"       -> rep_from_1\n",
    "         | NULL                 -> node_no_label\n",
    "         | \"(\" node \")\"\n",
    "\n",
    "    EDGE_LABEL: LABEL_STRING\n",
    "    NODE_LABEL: LABEL_STRING\n",
    "    NULL: \"?\"\n",
    "    LCASE_LETTER: \"a\"..\"z\"\n",
    "    UCASE_LETTER: \"A\"..\"Z\"\n",
    "    DIGIT: \"0\"..\"9\"\n",
    "\n",
    "    LETTER: UCASE_LETTER | LCASE_LETTER | DIGIT | \"_\" | \"-\"\n",
    "    LABEL_STRING: LETTER+ | \"_\" \n",
    "\n",
    "    %import common.CNAME -> NAME\n",
    "    %import common.WS_INLINE\n",
    "    %ignore WS_INLINE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@v_args(inline=True)    # Affects the signatures of the methods\n",
    "class CalculateTree(Transformer):\n",
    "    node_idx = 0\n",
    "    edge_idx = 0\n",
    "        \n",
    "    def node(self, name):\n",
    "        self.node_idx += 1\n",
    "        return \"(n{}:\".format(self.node_idx) + str(name) +\")\"\n",
    "    \n",
    "    def edge(self, name):\n",
    "        self.edge_idx += 1\n",
    "        return \"-[r{}:\".format(self.edge_idx) + str(name) +\"]-\"\n",
    "    \n",
    "    def node_no_label(self, name):\n",
    "        self.node_idx += 1\n",
    "        return \"(n{}\".format(self.node_idx) +\")\"\n",
    "    \n",
    "    def edge_no_label(self, name):\n",
    "        self.edge_idx += 1\n",
    "        return \"-[r{}\".format(self.edge_idx) +\"]-\"\n",
    "    \n",
    "    def edge_node(self, name1, name2):\n",
    "        path = name1 + name2\n",
    "        return path\n",
    "\n",
    "    def edge_right(self, name):\n",
    "        path = ''\n",
    "        path = name \n",
    "        return path + \">\"\n",
    "\n",
    "    def edge_left(self, name):\n",
    "        path = ''\n",
    "        path = name \n",
    "        return \"<\" + path\n",
    "\n",
    "    def rep_from_0(self, name):\n",
    "        self.node_idx += 1\n",
    "        path1 = \"(n{}:\".format(self.node_idx) + str(name) +\")\"\n",
    "        path2 = \"(n{}:\".format(self.node_idx) + str(name) +\")\" + \"--\" + \"(n{}:\".format(self.node_idx+1) + str(name) +\")\"\n",
    "        self.node_idx += 1\n",
    "        return ['', path1, path2]\n",
    "\n",
    "\n",
    "    def rep_from_1(self, name):\n",
    "        self.node_idx += 1\n",
    "        path1 = \"(n{}:\".format(self.node_idx) + str(name) +\")\"\n",
    "        path2 = \"(n{}:\".format(self.node_idx) + str(name) +\")\" + \"--\" + \"(n{}:\".format(self.node_idx+1) + str(name) +\")\"\n",
    "        self.node_idx += 1\n",
    "        return [path1, path2]\n",
    "\n",
    "    def node_or(self, name1, name2):\n",
    "        return [name1, name2]\n",
    "    \n",
    "    def edge_or(self, name1, name2):\n",
    "        return [name1, name2]\n",
    "    \n",
    "    def path_or(self, name1, name2):\n",
    "        return [name1, name2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permutes all possible pathways from the regular expression.\n",
    "def extractPathways(parse_tree):\n",
    "    all_elems = []\n",
    "    mlist = []\n",
    "    #Walks through the parse tree. If a node may have two or more labels\n",
    "    # it is added to our collection as a list of all possible labels.\n",
    "    for child in parse_tree.children:\n",
    "        if type(child) == str:\n",
    "            mlist.append([child])\n",
    "        else:\n",
    "            mlist.append(child)\n",
    "            \n",
    "    #We iterate through all possible combinations of node and edge labels\n",
    "    # along the provided regex.\n",
    "    for i in product(*mlist):\n",
    "        all_elems.append(list(i))\n",
    "        \n",
    "    #Filter null characters out of node labels.\n",
    "    pathways = []\n",
    "    for i in all_elems:\n",
    "        if('' in i): \n",
    "            idx = i.index(\"\")\n",
    "            i.pop(idx)\n",
    "            i.pop(idx-1)\n",
    "            pathways.append(i)\n",
    "        else:\n",
    "            pathways.append(i)\n",
    "    return pathways\n",
    "\n",
    "#Generates a CYPHER query for a regular expression. The first node in the regular expression will be \n",
    "# mapped onto the source_node_name.\n",
    "def getQueries(source_node_name, regexes):\n",
    "    all_queries = []\n",
    "    subgraph_nodes = []\n",
    "    # parse\n",
    "    regex_parser = Lark(regex_grammar, parser='lalr',transformer=CalculateTree())\n",
    "    parsed = regex_parser.parse(regexes)\n",
    "    all_pathways = extractPathways(parsed)\n",
    "    \n",
    "    queryStr = ''\n",
    "    final_idx = len(all_pathways)-1\n",
    "    for i, path in enumerate(all_pathways):\n",
    "        \n",
    "        if(path[-1]==''):path[-1]\n",
    "        start_node_num = path[0].split(\":\")[0].split(\"(\")[1].split(\")\")[0]\n",
    "        query = \"MATCH p1=\"\n",
    "        query += ''.join(str(elem) for elem in path)   \n",
    "        query += \" WHERE %s.name =\" % start_node_num\n",
    "        query += \" '%s'\" % source_node_name\n",
    "        \n",
    "        # add conditions to the node names if repeated types in the path\n",
    "        add_where = \"\"\n",
    "        repeated = []\n",
    "        for j in path:\n",
    "            if \":\" in j: # if type is given\n",
    "                if j.split(\":\")[1] in repeated: # if type is repeated\n",
    "                    if \")\" in j: # if it is a node\n",
    "                        prev = path[repeated.index(j.split(\":\")[1])].split(\":\")[0].split(\"(\")[1]\n",
    "                        current = j.split(\":\")[0].split(\"(\")[1]\n",
    "                        add_where += \" AND %s\" % prev\n",
    "                        add_where += \" <> %s\" % current\n",
    "                repeated.append(j.split(\":\")[1])\n",
    "            else:\n",
    "                repeated.append(\"?\")                \n",
    "            \n",
    "        query += add_where\n",
    "        query += \" WITH collect(p1) as nodez UNWIND nodez as c RETURN c\"\n",
    "        if(i==final_idx): queryStr += query\n",
    "        else: queryStr += query + \" UNION \"\n",
    "    \n",
    "    return queryStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing(regexes):\n",
    "    # parse\n",
    "    regex_parser = Lark(regex_grammar, parser='lalr',transformer=CalculateTree())\n",
    "    reg = regex_parser.parse\n",
    "    parsed = reg(regexes)\n",
    "    all_elems = extractPathways(parsed)\n",
    "\n",
    "    \n",
    "    llink = []\n",
    "    for i in all_elems:\n",
    "        Link = []\n",
    "        for j in i:\n",
    "            if ':' in j:\n",
    "                if '(' in j:\n",
    "                    nodes = j.split(':')[1].split(')')[0]\n",
    "                    Link.append(nodes)\n",
    "                if '[' in j:\n",
    "                    edges = j.split(':')[1].split(']')[0]\n",
    "                    Link.append(edges)\n",
    "            else:\n",
    "                edges = '?'\n",
    "                Link.append(edges)\n",
    "        #print(Link)\n",
    "\n",
    "        for i in range(math.floor(len(Link)/2)):\n",
    "            #print(i)\n",
    "            if i == 0:\n",
    "                llink.append(Link[i:(i+3)])\n",
    "            else:\n",
    "                llink.append(Link[(i*2):(i*2)+3])\n",
    "\n",
    "    return llink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubgraph_neo4j(graph_uri, source_node_name, regexes, compared_labels = None):\n",
    "    \n",
    "    queryStr = getQueries(source_node_name, regexes)    \n",
    "    \n",
    "    driver = GraphDatabase.driver(graph_uri)\n",
    "    \n",
    "    user_labels = []\n",
    "    for ele in parsing(regexes):\n",
    "        user_labels += ele\n",
    "    user_labels = list(set(user_labels))\n",
    "            \n",
    "    with driver.session() as session:\n",
    "        result = session.run(queryStr)\n",
    "        d = {}\n",
    "        join_values = []\n",
    "        for i in result.graph().nodes:\n",
    "            node_name = i['name']\n",
    "            if node_name not in join_values:\n",
    "                #print('labels = ',list(i.labels))\n",
    "                if len(i.labels)>1:\n",
    "                    for m in i.labels:\n",
    "                        if m in user_labels:\n",
    "                            node_type = m\n",
    "                        \n",
    "                        ### for multiple-labeled graph using regex \"? ? ?\"\n",
    "                        \n",
    "                        elif compared_labels != None:\n",
    "                            if m in compared_labels:\n",
    "                                node_type = m\n",
    "                        else:\n",
    "                            node_type = list(i.labels)[0]\n",
    "                        ###\n",
    "                else:\n",
    "                    node_type = list(i.labels)[0]\n",
    "                s = d.get(node_type,set())\n",
    "                s.add(node_name)\n",
    "                d[node_type] = s\n",
    "            join_values.append(node_name)\n",
    "\n",
    "        rels = set()\n",
    "        for i in result.graph().relationships:\n",
    "            start = i.start_node[\"name\"]\n",
    "            end = i.end_node[\"name\"]\n",
    "            rel_type = i.type\n",
    "            rels.add((start, end, rel_type))\n",
    "\n",
    "    raw_nodes = d        \n",
    "    edges = pd.DataFrame.from_records(list(rels),columns=[\"source\",\"target\",\"label\"])\n",
    "\n",
    "    data_frames = {}\n",
    "    for k in d:\n",
    "        node_names = list(d[k])\n",
    "        df = pd.DataFrame({\"name\":node_names}).set_index(\"name\")\n",
    "        data_frames[k] = df\n",
    "\n",
    "    sg = StellarDiGraph(data_frames,edges=edges, edge_type_column=\"label\")\n",
    "   \n",
    "    return sg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function. Constructs a dictonary; the keys are node names provided in\n",
    "# node_list. The values are the semantic subgraphs constructed using the \n",
    "# parameters G, semantic_query, and compare_labels.\n",
    "def buildSubgraphDictonaryForNodes(node_list, G, semantic_query, compared_labels):\n",
    "    subGs = {}\n",
    "    for node in node_list:\n",
    "        subG = getSubgraph_neo4j(G, node, semantic_query, compared_labels)\n",
    "        subGs[node] = subG\n",
    "    return subGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates random walks for various methods.\n",
    "def compactWalks(subgraph_dict, node_list, method, l, r, metapath = None):\n",
    "    subGs = {}\n",
    "\n",
    "\n",
    "    Walks = []\n",
    "    for node in node_list:\n",
    "        subG = subgraph_dict[node]\n",
    "        # DeepWalk\n",
    "        if method == 'deepwalk':\n",
    "            rw = UniformRandomWalk(subG) #BiasedRandomWalk(G)\n",
    "            walks = rw.run(\n",
    "                nodes= [node],#list(G.nodes()),  # root nodes\n",
    "                length = l,#adj_wlength,  # maximum length of a random walk\n",
    "                n = r #,  # number of random walks per root node\n",
    "                #seed = 1\n",
    "            )\n",
    "\n",
    "        # Node2Vec\n",
    "        elif method == 'node2vec':\n",
    "            rw = BiasedRandomWalk(subG)\n",
    "            walks = rw.run(\n",
    "                nodes= [node],  # root nodes\n",
    "                length = l,  # maximum length of a random walk\n",
    "                n = r,  # number of random walks per root node\n",
    "                p = 0.25,  # Defines (unormalised) probability, 1/p, of returning to source node\n",
    "                q = 0.25#,  # Defines (unormalised) probability, 1/q, for moving away from source node\n",
    "                #seed = 5\n",
    "            )\n",
    "\n",
    "        #Metapath2vec\n",
    "        elif method == 'metapath2vec':\n",
    "            rw = UniformRandomMetaPathWalk(subG)\n",
    "            walks = rw.run(\n",
    "                nodes= [node],#list(G.nodes()),  # root nodes\n",
    "                length = l,  # maximum length of a random walk\n",
    "                n = r,  # number of random walks per root node\n",
    "                metapaths = metapath#,\n",
    "                #seed = 5\n",
    "            )\n",
    "\n",
    "        # append walks\n",
    "        for w in walks:\n",
    "            Walks.append(w)\n",
    "            \n",
    "    return Walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates a model with embeddings from provided collection of Walks.\n",
    "def buildModel(Walks):\n",
    "    str_walks = [[str(n) for n in walk] for walk in Walks]\n",
    "    model = Word2Vec(str_walks, size=128, window=10, min_count=0, sg=1, workers=2, iter=5)\n",
    "    return model\n",
    "\n",
    "#Computes various benchmarks for a machine learning models.\n",
    "def evaluate(model, subgraph_dict, node_list1, node_list2):\n",
    "    evaluate_dict = {}\n",
    "    hit_at_1_in_list = 0\n",
    "    hit_at_3_in_list = 0\n",
    "    hit_at_5_in_list = 0\n",
    "    mrr_in_list = 0\n",
    "    \n",
    "    Node_List = node_list1 + node_list2\n",
    "    for n in node_list1:\n",
    "\n",
    "        print(\"==\", n, \"==\")\n",
    "        n_all = 0\n",
    "        num_in_list = 0\n",
    "        rank_in_list = 0\n",
    "\n",
    "        for i in model.wv.most_similar(n, topn = 20000):\n",
    "            n_all += 1\n",
    "\n",
    "            for j in Node_List:\n",
    "                if i[0] in subgraph_dict[j].nodes():\n",
    "                    nodeType = subgraph_dict[j].node_type(i[0])\n",
    "                    break\n",
    "\n",
    "            if i[0] in Node_List:\n",
    "                num_in_list += 1\n",
    "\n",
    "            if i[0] == node_list2[node_list1.index(n)]:\n",
    "                print('drugs* ',num_in_list)\n",
    "                # test: include only drugs in list\n",
    "                rank_in_list = num_in_list\n",
    "                if rank_in_list == 1:\n",
    "                    hit_at_1_in_list += 1\n",
    "                if rank_in_list <= 3:\n",
    "                    hit_at_3_in_list += 1\n",
    "                if rank_in_list <= 5:\n",
    "                    hit_at_5_in_list += 1\n",
    "                mrr_in_list += 1/rank_in_list\n",
    "\n",
    "    print('compute only in list:')\n",
    "    print(\"num of Compound*: \",num_in_list)\n",
    "    print(\"HIT@1 = \", round(hit_at_1_in_list/len(node_list1),4))\n",
    "    print(\"HIT@3 = \", round(hit_at_3_in_list/len(node_list1),4))\n",
    "    print(\"HIT@5 = \", round(hit_at_5_in_list/len(node_list1),4))\n",
    "    print(\"MRR = \", round(mrr_in_list/len(node_list1),4))\n",
    "    HIT1 = round(hit_at_1_in_list/len(node_list1),4)\n",
    "    HIT3 = round(hit_at_3_in_list/len(node_list1),4)\n",
    "    HIT5 = round(hit_at_5_in_list/len(node_list1),4)\n",
    "    MRR = round(mrr_in_list/len(node_list1),4)\n",
    "    \n",
    "    evaluate_dict['HIT@1'] = HIT1\n",
    "    evaluate_dict['HIT@3'] = HIT3\n",
    "    evaluate_dict['HIT@5'] = HIT5\n",
    "    evaluate_dict['MRR'] = MRR\n",
    "    \n",
    "    return evaluate_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Use Case - Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hetionet (Disease)\n",
    "\n",
    "node_list = [\"hypertension\",\"coronary artery disease\",\n",
    "\"endogenous depression\",\"panic disorder\",\n",
    "\"schizophrenia\",\n",
    "\"endogenous depression\",\n",
    "\"pancreatic cancer\",\"lung cancer\",\n",
    "\"breast cancer\",\"ovarian cancer\",\n",
    "\"bipolar disorder\",\"endogenous depression\",\n",
    "\"stomach cancer\",\"prostate cancer\",\n",
    "\"migraine\",\"epilepsy syndrome\"]\n",
    "\n",
    "\n",
    "node_pair1 = [\"hypertension\",\"endogenous depression\",\"schizophrenia\",\"endogenous depression\",\"pancreatic cancer\",\n",
    "              \"breast cancer\",\"bipolar disorder\",\"bipolar disorder\",\"stomach cancer\",\"migraine\"]\n",
    "\n",
    "\n",
    "node_pair2 = [\"coronary artery disease\",\"panic disorder\",\"panic disorder\",\"panic disorder\",\"lung cancer\",\n",
    "              \"ovarian cancer\",\"endogenous depression\",\"panic disorder\",\"prostate cancer\",\"epilepsy syndrome\"]\n",
    "\n",
    "\n",
    "semantic_query = \"Disease ASSOCIATES_DaG> Gene BINDS_CbG Compound\"\n",
    "\n",
    "\n",
    "subgraph_dict = buildSubgraphDictonaryForNodes(node_list, \"bolt://neo4j.het.io\", semantic_query, None)\n",
    "\n",
    "metapaths = [['Disease', 'Gene', 'Compound', 'Gene', 'Disease']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Walks = compactWalks(subgraph_dict, node_list, 'deepwalk', 80, 5)\n",
    "model = buildModel(Walks)\n",
    "\n",
    "# Retrieve node embeddings and corresponding subjects\n",
    "node_ids = model.wv.index2word  # list of node IDs\n",
    "node_embeddings = (\n",
    "    model.wv.vectors\n",
    ")\n",
    "\n",
    "evaluate(model, subgraph_dict, node_pair1, node_pair2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new drug pairs (only for clustering)\n",
    "node_list = ['Dexamethasone', 'Betamethasone', 'Hydrocortisone', 'Mometasone',\n",
    "'Canagliflozin', 'Dapagliflozin', #, 'Empagliflozin'\n",
    "'Lapatinib', 'Afatinib', 'Erlotinib', 'Gefitinib',\n",
    "'Captopril', 'Enalapril', 'Benazepril', 'Lisinopril',\n",
    "'Losartan', 'Valsartan', 'Telmisartan', 'Irbesartan',\n",
    "'Nifedipine', 'Felodipine', 'Amlodipine', 'Nicardipine',\n",
    "'Simvastatin', 'Atorvastatin', 'Fluvastatin', 'Lovastatin',\n",
    "'Alendronate', 'Incadronate', 'Zoledronate',\n",
    "'Citalopram', 'Escitalopram', 'Fluoxetine', 'Paroxetine', 'Sertraline',\n",
    "'Fluconazole', 'Voriconazole', 'Itraconazole', 'Ketoconazole'\n",
    "]\n",
    "label_true = [0,0,0,0,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5,6,6,6,6,7,7,7,8,8,8,8,8,9,9,9,9] # new piars\n",
    "\n",
    "semantic_query = \"Compound BINDS_CbG Gene ASSOCIATES_DaG< Disease\" \n",
    "\n",
    "subgraph_dict = buildSubgraphDictonaryForNodes(node_list, \"bolt://neo4j.het.io\", semantic_query, None)\n",
    "\n",
    "metapaths = [['Compound','Gene','Disease', 'Gene', 'Compound']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Walks = compactWalks(subgraph_dict, node_list, 'deepwalk', l = 80, r = 20)#, metapath = metapaths)\n",
    "model = buildModel(Walks)\n",
    "\n",
    "# Retrieve node embeddings and corresponding subjects\n",
    "node_ids = model.wv.index2word  # list of node IDs\n",
    "node_embeddings = (\n",
    "    model.wv.vectors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering\n",
    "tsne = TSNE(n_components=2)\n",
    "node_embeddings_2d = tsne.fit_transform(node_embeddings)\n",
    "\n",
    "elist = []\n",
    "for i in node_list:\n",
    "    elist.append(node_embeddings_2d[node_ids.index(i)])\n",
    "node_embeddings_drug_2d = np.array(elist)\n",
    "\n",
    "X = node_embeddings_drug_2d\n",
    "kmeans_fit = cluster.KMeans(n_clusters = 10).fit(X)\n",
    "cluster_labels = kmeans_fit.labels_\n",
    "\n",
    "label_pred = cluster_labels\n",
    "\n",
    "nmi = normalized_mutual_info_score(label_true, label_pred)\n",
    "nmi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
